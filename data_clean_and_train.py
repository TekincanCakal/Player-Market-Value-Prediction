import pandas as pd
import numpy as np
import re
import datetime
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error
from dotenv import load_dotenv
import os # Added for os.getenv

load_dotenv()

# (Tahmin bloÄŸu dosya sonuna taÅŸÄ±ndÄ±)
# ==============================================================================
# 1. AYARLAR VE VERÄ°YÄ° YÃœKLEME
# ==============================================================================
# Veri setinizin en son baÅŸarÄ±lÄ± olduÄŸu dosya adÄ±nÄ± kullanÄ±yoruz.
FILE_NAME = 'players_data.json' 
CURRENT_YEAR = datetime.date.today().year

# PyTorch iÃ§in cihazÄ± tanÄ±mla (CUDA kontrolÃ¼)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ”¥ PyTorch cihazÄ± ayarlandÄ±: {device}")

try:
    df = pd.read_json(FILE_NAME)
    print(f"âœ… '{FILE_NAME}' baÅŸarÄ±yla yÃ¼klendi. BaÅŸlangÄ±Ã§ Boyutu: {df.shape}")
    
    # SÃ¼tun isimlerini dÃ¼zeltme (Scraping Ã§Ä±ktÄ±sÄ±na gÃ¶re)
    df.rename(columns={'name': 'Name', 'Overall rating': 'Overall'}, inplace=True)

except FileNotFoundError:
    print(f"âŒ Hata: '{FILE_NAME}' dosyasÄ± bulunamadÄ±.")
    exit()

# ==============================================================================
# 2. Ã–ZELLÄ°K TEMÄ°ZLEME FONKSÄ°YONLARI
# ==============================================================================

def clean_currency(value):
    """'â‚¬', 'M' (milyon) ve 'K' (bin) iÃ§eren deÄŸerleri sayÄ±sal formata dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r."""
    if isinstance(value, str):
        if not value: return np.nan
        value = value.replace('â‚¬', '').strip()
        if 'M' in value:
            return float(value.replace('M', '')) * 1_000_000
        elif 'K' in value:
            return float(value.replace('K', '')) * 1_000
        try:
            return float(value)
        except ValueError:
            return np.nan
    return value

def clean_height_cm(height_str):
    """Santimetre (cm) deÄŸerini Ã§Ä±karÄ±r."""
    if isinstance(height_str, str) and 'cm' in height_str:
        match = re.search(r'(\d+)cm', height_str)
        if match:
            return float(match.group(1))
    return np.nan

def get_contract_end_year(contract_info):
    """SÃ¶zleÅŸme bitiÅŸ yÄ±lÄ±nÄ± Ã§Ä±karÄ±r."""
    if isinstance(contract_info, str) and '~' in contract_info:
        try:
            return int(contract_info.split('~')[-1].strip())
        except ValueError:
            return np.nan
    return np.nan

# ==============================================================================
# 3. VERÄ° TEMÄ°ZLEME VE DÃ–NÃœÅÃœM Ä°ÅLEMLERÄ°
# ==============================================================================

# Para Birimi, Boy ve SÃ¶zleÅŸme iÅŸlemleri
currency_cols = ['Value', 'Wage', 'Release clause']
for col in currency_cols:
    df[col] = df[col].astype(str).apply(clean_currency)
df['Height_cm'] = df['Height'].apply(clean_height_cm)
df.drop('Height', axis=1, inplace=True)
df[['Team', 'Contract_Info']] = df['Team & Contract'].str.split('\n', expand=True)
df['Contract_End_Year'] = df['Contract_Info'].apply(get_contract_end_year)
df.drop(['Team & Contract', 'Contract_Info'], axis=1, inplace=True)

# Ä°statistik sÃ¼tunlarÄ±nÄ± sayÄ±sallaÅŸtÄ±rma
cols_to_exclude_from_stat_conversion = ['Name', 'Position', 'Team', 'Best position', 'ID', 'Weight']
stat_cols = df.select_dtypes(include=['object']).columns.tolist()
stat_cols = [col for col in stat_cols if col not in cols_to_exclude_from_stat_conversion] 
for col in stat_cols:
    df[col] = pd.to_numeric(df[col], errors='coerce') 

# Gereksiz ve TekrarlÄ± SÃ¼tunlarÄ± KaldÄ±rma
cols_to_drop = ['', 'ID', 'Growth', 'Defending / Pace', 'Dribbling / Reflexes', 
                'Pace / Diving', 'Passing / Kicking', 'Shooting / Handling', 
                'Base stats', 'International reputation']
existing_cols_to_drop = [col for col in cols_to_drop if col in df.columns]
df.drop(existing_cols_to_drop, axis=1, inplace=True)
print("âœ… Temel temizlik adÄ±mlarÄ± tamamlandÄ±.")


# ==============================================================================
# 4. Ã–ZELLÄ°K MÃœHENDÄ°SLÄ°ÄÄ° (FEATURE ENGINEERING)
# ==============================================================================

df['Age_Squared'] = df['Age'] ** 2
df['Contract_Duration'] = df['Contract_End_Year'] - CURRENT_YEAR
df['Contract_Duration'] = df['Contract_Duration'].apply(lambda x: max(0, x))
print("âœ… Yeni Ã¶zellikler oluÅŸturuldu.")


# ==============================================================================
# 5. EKSÄ°K DEÄER YÃ–NETÄ°MÄ° VE TEKÄ°L SÃœTUN TEMÄ°ZLÄ°ÄÄ°
# ==============================================================================

# SayÄ±sal Doldurma (Medyan) ve Tamamen NaN SÃ¼tunlarÄ± KaldÄ±rma
numerical_cols = df.select_dtypes(include=np.number).columns
for col in numerical_cols:
    if df[col].isnull().all():
        df.drop(col, axis=1, inplace=True)
    else:
        df[col] = df[col].fillna(df[col].median())

# Kategorik Doldurma ('Unknown')
categorical_cols = df.select_dtypes(include=['object']).columns
for col in categorical_cols:
    if col not in ['Name', 'Weight']: 
        df[col] = df[col].fillna('Unknown')

# Kategorik Kodlama (One-Hot Encoding)
cols_to_encode = ['Position', 'Team', 'Best position']
existing_cols_to_encode = [col for col in cols_to_encode if col in df.columns]
# One-Hot Encoding (dtype=int ile sayÄ±sal olmasÄ±nÄ± garanti et)
df_final = pd.get_dummies(df, columns=existing_cols_to_encode, drop_first=True, dtype=int)

# Son Temizlik
if 'Name' in df_final.columns:
    df_final.drop('Name', axis=1, inplace=True)
if 'Weight' in df_final.columns:
    df_final.drop('Weight', axis=1, inplace=True)

print(f"âœ… Veri temizliÄŸi ve kodlama bitti. Son Boyut: {df_final.shape}")


# ==============================================================================
# 6. TRAIN/TEST VE PYTORCH TENSOR'LARA DÃ–NÃœÅÃœM (LOG DÃ–NÃœÅÃœMLÃœ)
# ==============================================================================

X = df_final.drop('Value', axis=1)
Y = df_final[['Value']] 

# TÃ¼m X verisini float'a Ã§evir (Scaler hatasÄ±nÄ± Ã¶nlemek iÃ§in kritik)
X = X.astype(float)

variance = X.var()
constant_columns = variance[variance == 0].index.tolist()
if constant_columns:
    X.drop(columns=constant_columns, inplace=True)

# EÄŸitim ve Test Setlerine BÃ¶lme
X_train_df, X_test_df, Y_train_df, Y_test_df = train_test_split(
    X, Y, test_size=0.2, random_state=42
)

# --- X Ã–zelliklerini Ã–lÃ§eklendirme ---
# TÃ¼m sÃ¼tunlarÄ± Ã¶lÃ§ekle (Frontend ile 1-1 eÅŸleÅŸme iÃ§in ÅŸart)
x_scaler = StandardScaler()
X_train_df[:] = x_scaler.fit_transform(X_train_df)
X_test_df[:] = x_scaler.transform(X_test_df)
print("âœ… X Ã–zellikleri Ã¶lÃ§eklendirildi.")


# --- Y Hedef DeÄŸiÅŸkeni Logaritmik DÃ¶nÃ¼ÅŸÃ¼m ve Ã–lÃ§eklendirme ---
# Log dÃ¶nÃ¼ÅŸÃ¼mÃ¼ (np.log1p) ile daÄŸÄ±lÄ±mÄ± normalize et ve negatif tahminleri engelle
y_scaler = StandardScaler()
Y_train_log = np.log1p(Y_train_df) 
Y_test_log = np.log1p(Y_test_df)

Y_train_scaled = y_scaler.fit_transform(Y_train_log)
Y_test_scaled = y_scaler.transform(Y_test_log)
print("âœ… Y Hedef DeÄŸiÅŸkeni Logaritmik DÃ¶nÃ¼ÅŸÃ¼m ve Ã–lÃ§ekleme yapÄ±ldÄ±.")


def enforce_float(df):
    """DataFrame'deki tÃ¼m sÃ¼tunlarÄ± float'a zorlar."""
    df = df.apply(pd.to_numeric, errors='coerce')
    return df.fillna(df.mean())

X_train_df = enforce_float(X_train_df.copy())
X_test_df = enforce_float(X_test_df.copy())


# NumPy'a ve sonra PyTorch Tensor'lara dÃ¶nÃ¼ÅŸtÃ¼rme
X_train_tensor = torch.tensor(X_train_df.values.astype(np.float32)).to(device)
Y_train_tensor = torch.tensor(Y_train_scaled.astype(np.float32)).to(device)
X_test_tensor = torch.tensor(X_test_df.values.astype(np.float32)).to(device)
Y_test_tensor = torch.tensor(Y_test_scaled.astype(np.float32)).to(device)

# DataLoader oluÅŸturma
train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) # Batch size artÄ±rÄ±ldÄ±

print("âœ… Veriler PyTorch Tensor'lara dÃ¶nÃ¼ÅŸtÃ¼rÃ¼ldÃ¼ ve DataLoader hazÄ±rlandÄ±.")


# ==============================================================================
# 7. PYTORCH MODEL TANIMI (DROPOUT DÃœZELTÄ°LDÄ°)
# ==============================================================================

class PlayerValueModel(nn.Module):
    def __init__(self, input_size):
        super(PlayerValueModel, self).__init__()
        self.fc1 = nn.Linear(input_size, 256) 
        self.fc2 = nn.Linear(256, 128)
        self.fc3 = nn.Linear(128, 64)
        self.fc4 = nn.Linear(64, 1)
        self.relu = nn.ReLU()
        # Dropout %30'dan %10'a dÃ¼ÅŸÃ¼rÃ¼ldÃ¼
        self.dropout = nn.Dropout(0.1) 
        
    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.relu(self.fc3(x))
        x = self.fc4(x)
        return x

# Model baÅŸlatma
input_size = X_train_tensor.shape[1]
model = PlayerValueModel(input_size).to(device)

# KayÄ±p Fonksiyonu ve Optimizasyon
criterion = nn.MSELoss() 
# Ã–ÄŸrenme OranÄ± 0.0005'ten 0.0001'e dÃ¼ÅŸÃ¼rÃ¼ldÃ¼ (KararlÄ±lÄ±k iÃ§in)
optimizer = optim.Adam(model.parameters(), lr=0.0001) 

print(f"\nğŸ§  Yapay Sinir AÄŸÄ± ({input_size} GiriÅŸ) Modeli HazÄ±rlandÄ±.")


# ==============================================================================
# 8. MODEL EÄÄ°TÄ°M DÃ–NGÃœSÃœ
# ==============================================================================

NUM_EPOCHS = 1200 # Epoch sayÄ±sÄ± biraz daha artÄ±rÄ±ldÄ±

print(f"\nâ³ Model {NUM_EPOCHS} epoch boyunca eÄŸitiliyor...")

for epoch in range(NUM_EPOCHS):
    model.train() 
    for inputs, targets in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
        
    if (epoch+1) % 50 == 0:
        print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {loss.item():.6f}')

print("âœ… EÄŸitim tamamlandÄ±.")


# ==============================================================================
# 9. MODEL DEÄERLENDÄ°RME (LOG DÃ–NÃœÅÃœMÃœ GERÄ° ALINDI)
# ==============================================================================

model.eval() # DeÄŸerlendirme moduna geÃ§
with torch.no_grad():
    Y_pred_tensor = model(X_test_tensor)
    
    # Ã–lÃ§eklenmiÅŸ tahminleri CPU'ya taÅŸÄ±
    Y_pred_scaled = Y_pred_tensor.cpu().numpy()
    Y_test_scaled = Y_test_tensor.cpu().numpy()

# 1. AdÄ±m: Ã–lÃ§eklemeyi orijinal logaritmik aralÄ±ÄŸa geri al
Y_pred_unscaled_log = y_scaler.inverse_transform(Y_pred_scaled)
Y_test_unscaled_log = y_scaler.inverse_transform(Y_test_scaled)

# 2. AdÄ±m: Log dÃ¶nÃ¼ÅŸÃ¼mÃ¼nÃ¼ (np.expm1) geri alarak orijinal Euro deÄŸerine dÃ¶n
Y_pred = np.expm1(Y_pred_unscaled_log)
Y_test = np.expm1(Y_test_unscaled_log)


# PerformansÄ± DeÄŸerlendirme (MAE)
mae = mean_absolute_error(Y_test, Y_pred)

print("\n" + "="*50)
print("ğŸš€ OPTÄ°MÄ°ZE PYTORCH MODEL DEÄERLENDÄ°RME SONUÃ‡LARI")
print(f"Test Seti Ãœzerindeki Ortalama Mutlak Hata (MAE): â‚¬{mae:,.2f}")
print("==================================================")

# Ã–rnek tahminleri gÃ¶ster
sample_predictions = pd.DataFrame({
    'GerÃ§ek DeÄŸer': Y_test.flatten(), 
    'Tahmin Edilen DeÄŸer': Y_pred.flatten()
})
sample_predictions['Hata'] = abs(sample_predictions['GerÃ§ek DeÄŸer'] - sample_predictions['Tahmin Edilen DeÄŸer'])
print("\nğŸ“ Sinir AÄŸÄ±nÄ±n Ã–rnek Tahminleri (Ä°lk 5):")
print(sample_predictions.head().sort_values(by='GerÃ§ek DeÄŸer', ascending=False))


# ==============================================================================
# 10. VERCEL ENTEGRASYONU: DB LOGLAMA VE JSON EXPORT
# ==============================================================================

import json
import os
import psycopg2
from datetime import datetime

def export_model_to_json(model, x_scaler, y_scaler, input_columns):
    """
    Modelin aÄŸÄ±rlÄ±klarÄ±nÄ± (Weights) ve Scaler parametrelerini 
    JavaScript tarafÄ±nda kullanÄ±lmak Ã¼zere JSON formatÄ±na Ã§evirir.
    """
    state_dict = model.state_dict()
    
    # PyTorch TensorlarÄ±nÄ± listeye Ã§evir
    weights = {}
    for key, value in state_dict.items():
        weights[key] = value.cpu().numpy().tolist()
        
    export_data = {
        "timestamp": datetime.now().isoformat(),
        "model_weights": weights,
        "x_scaler_mean": x_scaler.mean_.tolist(),
        "x_scaler_scale": x_scaler.scale_.tolist(),
        "y_scaler_mean": y_scaler.mean_.tolist(),
        "y_scaler_scale": y_scaler.scale_.tolist(),
        "input_columns": input_columns.tolist(),
        "mae_score": float(mae)
    }
    
    return json.dumps(export_data)

def save_to_postgres(json_data, mae_score, loss):
    """
    EÄŸitim sonuÃ§larÄ±nÄ± ve model JSON'Ä±nÄ± Vercel Postgres'e kaydeder.
    """
    db_url = os.environ.get("POSTGRES_URL")
    if not db_url:
        print("âš ï¸ POSTGRES_URL bulunamadÄ±, DB'ye kayÄ±t yapÄ±lmadÄ±. (Sadece yerel test mi?)")
        # Yerel test iÃ§in dosyaya yaz
        with open("model_export.json", "w", encoding="utf-8") as f:
            f.write(json_data)
        print("âœ… Model 'model_export.json' olarak yerel diske kaydedildi.")
        return

    try:
        conn = psycopg2.connect(db_url)
        cur = conn.cursor()
        
        # TablolarÄ± oluÅŸtur (Yoksa)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS training_logs (
                id SERIAL PRIMARY KEY,
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                mae_score FLOAT,
                final_loss FLOAT,
                model_json JSONB
            );
        """)
        
        # Veriyi ekle
        cur.execute("""
            INSERT INTO training_logs (mae_score, final_loss, model_json)
            VALUES (%s, %s, %s)
        """, (mae_score, loss, json_data))
        
        conn.commit()
        cur.close()
        conn.close()
        print("âœ… EÄŸitim sonuÃ§larÄ± ve Model JSON Vercel Postgres'e baÅŸarÄ±yla kaydedildi!")
        
    except Exception as e:
        print(f"âŒ DB KayÄ±t HatasÄ±: {e}")

# JSON Verisini HazÄ±rla
print("ğŸ”„ Model JSON formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor...")
# Input columns sÄ±ralamasÄ±nÄ± kaydetmek Ã¶nemli (Frontend'de aynÄ± sÄ±rayla Ã¶zellik vektÃ¶rÃ¼ oluÅŸturmak iÃ§in)
input_cols = X.columns 

model_json = export_model_to_json(model, x_scaler, y_scaler, input_cols)

# DB'ye Kaydet
# Not: 'loss' deÄŸiÅŸkeni eÄŸitim dÃ¶ngÃ¼sÃ¼nden gelen son loss deÄŸeridir.
final_loss = loss.item() if 'loss' in locals() else 0.0
save_to_postgres(model_json, mae, final_loss)

# ==============================================================================
# 11. TAHMÄ°NLERÄ° OLUÅTUR VE VERÄ°TABANINA KAYDET (YENÄ° Ã–ZELLÄ°K)
# ==============================================================================
print("TÃ¼m oyuncular iÃ§in tahminler oluÅŸturuluyor...")

# TÃ¼m veri seti Ã¼zerinde tahmin yap (X_scaled kullanacaÄŸÄ±z)
# X zaten float olarak ayarlanmÄ±ÅŸtÄ±.
# TÃ¼m veriyi tekrar scale et (EÄŸitimde fit edilen scaler ile)

X_all_scaled = x_scaler.transform(X)
X_tensor_all = torch.tensor(X_all_scaled, dtype=torch.float32).to(device)

model.eval()
with torch.no_grad():
    predictions_log = model(X_tensor_all).cpu().numpy()

# Ters Log DÃ¶nÃ¼ÅŸÃ¼mÃ¼ (GerÃ§ek DeÄŸer Tahmini)
# Ã–nce Scaler'Ä±n tersini al (Z-score -> Log Value)
predictions_rescaled = y_scaler.inverse_transform(predictions_log)
# Sonra Log'un tersini al (Log Value -> Euro)
predictions_eur = np.expm1(predictions_rescaled)

# SonuÃ§larÄ± DataFrame'e ekle
# Orjinal DataFrame'den isim ve diÄŸer bilgileri alabilmek iÃ§in, indexleri kullanmamÄ±z lazÄ±m.
# df_final Ã¼zerinde Ã§alÄ±ÅŸmÄ±ÅŸtÄ±k ama isimleri atmÄ±ÅŸtÄ±k.
# En baÅŸta 'df' deÄŸiÅŸkeninde orjinal veri duruyor.

# df ile X aynÄ± indexe sahip olmalÄ±.
df_results = df.copy() 
# Temizlik sÄ±rasÄ±nda satÄ±r atÄ±ldÄ±ysa indexler kaymÄ±ÅŸ olabilir, ancak biz dropna yaptÄ±k.
# En garantisi: df_final'in indexlerini kullanmak.
df_results = df_results.loc[df_final.index]

df_results['Predicted_Value'] = predictions_eur
df_results['Value_Diff'] = df_results['Predicted_Value'] - df_results['Value']

# Kaydedilecek sÃ¼tunlarÄ± seÃ§
# 'Name' sÃ¼tunu df'de mevcut.
columns_to_save = ['Name', 'Age', 'Overall', 'Potential', 'Value', 'Predicted_Value', 'Value_Diff']
# EÄŸer 'Nationality' veya 'Team' gibi ek bilgiler istersek buraya ekleyebiliriz.

# Sadece bu sÃ¼tunlarÄ± al
df_save = df_results[columns_to_save].copy()

# DeÄŸerleri tam sayÄ±ya yuvarla
df_save['Value'] = df_save['Value'].astype(int)
df_save['Predicted_Value'] = df_save['Predicted_Value'].astype(int)
df_save['Value_Diff'] = df_save['Value_Diff'].astype(int)

# VeritabanÄ±na Kaydet (players tablosu)
try:
    from sqlalchemy import create_engine, text
    
    # SQLAlchemy motoru oluÅŸtur (Postgres)
    # POSTGRES_URL: postgresql://admin:admin@localhost:5432/football_db
    # SQLAlchemy iÃ§in 'postgresql+psycopg2://...' formatÄ± gerekebilir veya 'postgresql://' yeterli.
    db_url = os.getenv("POSTGRES_URL").replace("postgresql://", "postgresql+psycopg2://")
    engine = create_engine(db_url)
    
    # Tabloyu (varsa) Ã¼zerine yaz
    df_save.to_sql('players', engine, if_exists='replace', index=False)
    
    # Primary Key ekle (Opsiyonel ama iyi olur)
    # with engine.connect() as con:
    #     con.execute(text('ALTER TABLE players ADD PRIMARY KEY ("Name");'))
        
    print(f"âœ… {len(df_save)} oyuncu verisi 'players' tablosuna kaydedildi.")
    
except Exception as e:
    print(f"âŒ Oyuncu verileri kaydedilemedi: {e}")